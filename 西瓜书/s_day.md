# 西瓜书第二次打卡
## 第四章 决策树（或称判定树）

### 4.1 基本流程
基于树结构进行决策

一般决策树包含一个根结点、若干个内部结点和若干个叶结点。

- 叶结点对应着决策属性
- 其他每个节点对应着一个属性测试，每个结点包含的样本集合根据属性测试的结果被划分到子结点中。
- 根结点包含样本全集

决策树学习的目的是为了产生一颗泛化能力强，即处理未见实例能力强的决策树，其基本流程遵循简单且只管饿“分而治之”（divide-and-conquer）策略

输入：训练集 $D = ${$(x_1,y_1),(x_2,y_2),\dots,(x_m,y_m)$}

&emsp;&emsp;&emsp;属性集：$A = ${$a_1,a_2,\dot , a_d$};

过程：函数TreeGenerate(D,A)

1. 生成结点node；
2. **if** D 中样本权属于同一个类别C **then**
3. &emsp;&emsp;将node标记为C类叶结点；**return**
4. **end if**
5. **if** $A = \emptyset$ **OR** D中样本在A上的取值相同 **then**
6. &emsp;&emsp;将node标记为叶结点，其类别标记为D中样本数组多的类；**return**
7. **end if**
8. 从A中选择最优化分属性$a_*$;
9. **for** $ a_* $ 
10. &emsp;&emsp;的每一个值 $ a_{*}^v$ **do**
11. &emsp;&emsp;为node生成一个分支；令$D_v$表示D中在$a_*$上取值为
12. &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$a_*^v$的样本子集
13. &emsp;&emsp;**if** $D_v$为空，**then**
14. &emsp;&emsp;&emsp;&emsp;将分支结点标记为叶结点，其类别标记为D中样本最多的类；**return**
15. &emsp;&emsp;**else**
16. &emsp;&emsp;&emsp;&emsp;以$TreeGenerate(D_v,a $ \ {$a_*$}$ )$为分支结点
17. &emsp;&emsp;**end if**
18. **end for**

输出： 以node为根的一颗决策树

三种情形导致递归返回

- 1.当前结点包含的样本全属于同一个类别，无需划分
- 2.当前属性集为空，或是所有样本在所有属性上取值相同，无法划分
- 3.当前结点包含的样本集合为空，不能划分。

### 4.2 划分选择
我们希望决策树的分支结点所包含的样本尽可能属于同一类别，也就是“纯度”（purity）越来越高

#### 4.2.1 信息增益

信息熵(information entropy)，度量纯度的最常用的一种指标。

- D：样本集合
- $p_k$第k类样本所占的比例（k = 1，2，……，|y|）

$$Ent(D) = -\sum_{k=1}^{|y|}pklog_2p_k$$

Ent(D)的值越小纯度越高

假设离散属性a有V个可能的取值{$a^1,a^2,\dots , a^V$},如果使用a来对样本集进行划分，那么将会产生V个分支结点，第v个分支结点包含了D中所有在属性a上取值为$a^v$的样本，记为$D^v$,考虑到不同的分支结点锁包含的样本数不同，给分支结点赋予权重$\frac{|D^v|}{|D|}$,即样本数越多的分支结点的影响越大，此时可计算出用属性a对样本集D进行划分所获得的“信息增益”（information gain）

$$Gain(D,a) = Ent(D) - \sum_{v=1}^V \frac{|D^v|}{D} * Ent(D^v)$$

一般而言信息增益越大，意味着使用属性a进行划分的纯度越高，所以可以使用信息增益来进行决策树的划分属性选择，即选择

$a_{*} = $ 

$\mathop{\arg\min}_{a \in A} \ \ Gain(D,a)$

**ID3决策树算法**是以信息增益来对决策树进行划分的。

#### 4.2.2 增益率

信息增益对于取值数目比较多的属性有多偏好，为了减少这种偏好带来的不利影响，C4.5决策算法不直接使用信息增益而是直接使用“增益率”（gain ratio）来选择最优的划分。

$$Gain_ratio(D,a) = \frac{Gain(D,a)}{IV(a)}$$

其中

$$IV(a) = -\sum_{v=1}^V \frac{|D^v|}{D}log_2\frac{D^v}{D}$$

被称为a的“固有值”(intrinsic value)，属性a的可能取值数目越多（也就是V越大），则IV(a)的值通常越大。

增益率可能对数目比较少的有所偏好。所以C4.5不是直接选择增益率最大的候选划分属性，而是使用了一个启发式：先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的。

#### 4.2.3 基尼系数

CART决策树使用“基尼系数”来划分属性。采用和之前一样的符号，数据集D的纯度可以用基尼值来度量。

<center>![](./picture/1.png)</center>


$$= 1 - \sum_{k=1}^{|y|}p_k^2$$

Gini(D)反映了从数据集D中随机抽取两个样本，其类别标记不一致的概率，因为，Gini(D)越小，则数据集D的纯度越高

属性a的基尼指数定义为：

$$Gini_index(D,a) = \sum_{v = 1}^{V} \frac{|D^v|}{|D|} Gini(D^v)$$

划分的时候选取基尼指数最小的属性作为划分属性，也就是

$$\mathop{\arg\min}_{a \in A} \ \ Gain_index(D,a)$$

### 4.3 剪枝处理
剪枝(pruning)是决策树中防止“过拟合”的主要手段。

剪枝策略分为“预剪枝”(prepuing)和“后剪枝”(postpruning)

- 预剪枝是指在决策树生成过程中，对每个结点在划分前进行评估，若当前结点的划分不能带来决策树泛化性能的提升，则停止划分并将当前结点标记为叶结点。
- 后剪枝则是先从训练集生成一个完整的决策树，然后自底向上地对非叶节点进行考察，若该结点对应地字数替换成叶结点能够使泛化性能能提升，则将该子树替换为叶结点。

### 4.4 连续与缺失值

#### 4.4.1连续值处理

给定样本集 D 和连续属性 α，假定 α 在 D 上出现了 n 个不同的取值，将这 些值从小到大进行排序，记为 {$α_1 , α_2, . . ， a_n$}. 基于划分点 t 可将 D 分为子集$D_t^- 和 D_t^+$ 其中 $D_t^-$ 包含那些在属性 α 上取值不大于 t 的样本,而 $D_t^-$ 则包含那些在属性 α 上取值大于 t 的样本.显然对相邻的属性取值 $a_i 与 α_{i+l}$ 来说，t在区间[$a^i， a^{i+l}$) 中取任意值所产生的划分结果相同.因此，对连续属性 α 我们 可考察包含 n - 1 个元素的候选划分点集合$


$$T_a =  \frac{a^i + a^{i+1}}{2} |1 \leq i \leq n - 1$$ （等号右边要加大括号）

也就是说把区间[$a^i， a^{i+l}$) 的中位点$\frac{a^i + a^{i+1}}{2}$作为划分点。


<center>![](./picture/2.png)</center>

其中 Gain(D， a, t) 是样本集 D 基于划分点 t 划分后的信息增益.于是，我们就可选择使 Gain(D， α， t) 最大化的划分点.


#### 4.4.2 缺失值处理

需要解决两个问题

- 1. 如何在属性值缺失的条件下进行划分
- 2. 给定划分属性，若样本在该属性上的值缺失，如何对样本进行划分。

#### 对于问题1：


$D^~$（顶头有一~）为没有属性确实的样本子集
<center>![](./picture/3.png)</center>

我们可以得到信息增益的推广形式

<center>![](./picture/4.png)</center>

#### 对于问题2：

若样本 x 在划分属性 α 上的取值己知,则将 x 划入与其取值对应的子结点，且样本权值在于结点中保持为 $w_x$. 若样本 x 在划分属性 α 上的取值未知，则将 x 同时划入所有子结点,且样本权值在与属性值 $a^v$ 对应的子结点中调整为(顶头有一~）$r^~ * w_x $，直观地看，这就是让同一个样本以不同的概率划入到不同的子结点中去.


### 4.5 多变量决策树

和单变量的相似，但是可以采用斜边的形式将模型简化（就是简单划线）。