# 西瓜书第一次打卡
# 第一章绪论
## 1.1 引言
> 如果说计算机科学是研究关于“算法”的学问，那么类似的，可以说机器学习是研究关于“学习算法”的学问

## 1.2 基本术语
根据是否有标签分为有监督学习（有标签）和无监督学习（无标签）。

泛化能力
> 使模型适用于新的样本的能力

## 1.3假设空间
归纳和演绎是科学推理的两大基本手段

归纳学习

 - 广义归纳学习：大体相当于从样例中学习
 - 侠义归纳学习：要求从训练数据中学得概念

版本空间：
> 可能有多个假设和训练集一直，即存在一个与训练集一直的“假设集合” 

对于版本空间的思考，我还写可以写放置于[这里](./关于版本空间的思考.docx)

## 1.4 归纳偏好
归纳偏好（偏好）
> 机器学习算法再学习过程中对某种类型假设的偏好

奥卡姆剃刀
> 若有多个假设与观察一致，则选最简单的那个

没有免费午餐定理（NFL）
> 无论a多么聪明、b算法多么笨拙，ab两算法最后的期望的性能误差是相等的。

# 第三章 线性模型

## 3.1 基本形式

$$ f(x) = w^T x + b $$ 

## 3.2 线性回归

序（order）
> 可以将离散值转化为连续值。比如（高，中，低）= （1.0，0.5，0.0）

线性回归试图得到：

$$ f(x_i) = wx_i + b 使得 f(x_i) 近似于y_i $$

我们可以使用均方误差来进行度量，而我们做的就是试图让均方误差最小化，也就是：
 
![](./picture/squre_loss.jpg)

对于w和b的优化

![](./picture/1.jpg)

### 对于多元线性回归

$$f(x_i) = w^T x_i + b 使得 f(x_i) 近似于y_i  （此处的x_i、w都是矩阵、向量，上面的式子不是）$$

![](./picture/2.jpg)

而在实际的时候我们经常遇到$X^TX$并不是满秩的，因为现实中经常会出现类别的数量甚至对于实际数，这回到这了线性方程组的解并不是唯一的，而这个时候对于选择哪一个解作为输出，一般有学习算法的归纳偏好所来决定，而常见的做法是引入正则项。

线性回归这一版到这里还没有结束，有的时候我们可能会进行一些转化，这也是线性回归的衍生。比如：
$$lny = w^T * x + b$$

这就是对数几率回归，它实际上想要让$e^{w^T*x+b}$逼近y，虽然仍是线性回归，但实质上已经是在对于求取输入空间到输出空间的非线性函数映射。


### 广义线性模型

跟一般的可以考虑单调可微函数$g(.)$,令
$$y = g(.)^{-1} * (w^T * x + b)$$

其中$g(.)$被称为“联系函数”，显然对数线性回归是在广义线性模型在$g(.) = ln(.)$时候的特例。

## 3.3 对数几率回归

对于二分类问题，线性回归模型产生的预测值$A = w ^ T * x +b$,这个时候我们将实值Z转化为0/1.最理想的函数时单位阶跃函数，但是单位阶跃函数并不连续，所以我们希望找到一个在一定程度上可以代替单位阶跃函数的代替函数，也就是如下的函数，对数几率函数：
$$y = \frac{1}{1 + e^{-(w^T + b)}}$$

把它的图像画出来，可以看得出他是一个Sigmod函数（S形函数）

将y作正例的几率，那么1-y就是反例的几率，

$\frac{y}{1-y}$反应了x的作为正例的可能性。

我们有$$ln(\frac{y}{1-y})$$。

而我们做的就是对这一个值进行逼近，我们要对w、b进行确定。

在此将y在成后验机率的形式也就是
$$p(y = 1|\overrightarrow{x}) = y$$

则
$$p(y = 0|\overrightarrow{x}) = 1- y$$

原本的式子可以转化为

$$ln(\frac{p(y = 1|\overrightarrow{x}}{p(y = 0|\overrightarrow{x}}) = \overrightarrow{w}^T * \overrightarrow{x} + b$$

我们可以得到， 

$$p(y = 1|\overrightarrow{x}) = \frac{e^{\overrightarrow{w}^T * \overrightarrow{x} + b}}{1 + e^{\overrightarrow{w}^T * \overrightarrow{x} + b}}$$

$$p(y = 0|\overrightarrow{x}) = \frac{1}{1 + e^{\overrightarrow{w}^T * \overrightarrow{x} + b}}$$

我们使用极大似然估计w和b。给定数据集{$(x_i,y_i)$}$_{i=1}^m$,对率回归模型最大化“对数似然”

$$l(\overrightarrow{w},b) = \sum_i^m {ln(p(y_i|x_i;\overrightarrow{w},b))}$$

![](./picture/logit.jpg)

### 3.4 线性判别法
> 线性判别分析(Linear Discriminant Analysis,简称LDA)。

> 戈丁训练样例集，设法将样例投影到一条直线上，使得同类样例的拖影点尽可能接近、异类样例的拖影点尽可能远离；在对新样本进行分类的时候，将其投影到同样的这条直线上是，再更根据投影点的位置来确定新样本的类别。


![](./picture/LDA-2.jpg)
![](./picture/LDA-1.jpg)

对于多分类的可以进行类似的完成。

### 3.5 多分类学习

我们对多酚类任务进行拆分，然后对多个分类请进行集成。

经典分类策略有三种：

- 一对一（OvO）
> 一对一将N个类别进行两两配对，从而产生N（N-1）/2个二分类任务。
- 一对剩余（OvR）
> 每次将一个类的样例作为正例、所有其他类的样例作为反剋来训练N个分类器。
- 多对多(MvM)
> MvM是每次将若干个作为正类、若干个其他类作为反类。对于MvM而言，它的正反类构造必须有特殊的设计，不能随意选取。

一种MvM中常用的技术，纠错输出码（ECOC）

- 编码：对N个类分别做M次划分，每次划分将一部分类划为正类，一部分划为反类，从而形成一个二分类训练集；这样子一共产生了M个训练集，可以训练出M个分类器
- 解码：M个分类器分别对测试样本进行预测，这些预测标记组成一个编码。将这些编码与每个类别各自的编码进行比较，返回其中距离最小的类别最为预测结果。

ECOC编码越长，对于纠错能力越强，但是编码越长，计算的开销会越大；对于有限类别，可能的组合数是有限的，，码长超过一定范围之后就失去了意义。

对于同等长度的编码，理论上任意两个类别间的编码距离越远，纠错能力越强，计算最优编码是NP难问题。
> 但是一般实际使用的时候并不是编码的理论性质越好，分类性能就越好的。


### 3.6 类别不平衡问题。

解决方法：

- 欠采样：
> 对于多的类别进行次数比较少的采样。
- 过采样
> 对于次数比较少的类别进行多次的重复采样。
- 阈值移动
> 在决策的时候乘上$\frac{m-}{m+}$,m-为反例数目，m+为正例数目

直接进行过采样会出现问题，一般采用SMOTE算法，对于相应的例子进行插值，产生额外的正例。

欠采样也是会出现问题，所以可以使用EasyEnsemble使用集成学习机制，将多的类别划分为若干个集合给不同的学习器使用，这样子即进行了欠采样，又不会损失重要信息。

