# 李宏毅打卡（第二天）

## P4

错误来自哪里？

- 偏置
- 权值

Bias and Variance of Estimator


使用比较简单的模型的时候它的方差是非常小的

- 为什么比较复杂的模型比较差呢？
- 比较简单的模型，受到数据的影响比较小


对于一个简单的模型，那么它的bias就比较小，如果不包含真正的u，那么无论如何取都取不到u；但是模型比较复杂的时候，范围比较大，可能包含我们的目标u，但是因为给的数据集不够大，无法确认它具体在哪里。

如果是一个错误是来自于variance比较大的话，就叫做overfitting，如果bias比较大的话，来自于underfitting

如何知道是bias大还是variance大呢？
（看是underfitting还是overfitting）

Variance太大怎么处理？

- 增加数据（数据增强或者采集多点的数据）
- 正则化，使得参数越小越好，使得原本的曲线更加平滑

Model Selection

- 使得bias和variance最小，但是多次不能够在训练集上进行多次训练取最小误差的那个函数。
- 我们可以将训练集分为一个训练集和一个验证集
> 类似于使用多折交叉验证的方法
> 
- woc，这里后面也讲了多折交叉验证，那就是多折交叉验证了。


P5

Grandient Descent

![](./picture/gd.png)

学习率

![](./picture/dglr.png)

最好把learning rate时候的画出来，确定一下是什么问题。

Adagrad

- 对于每个参数都一个learning rate
- 每个参数的学习率都除以原本的学习率的开方均值
> 设a_i 为第i个学习率，那么a_i = \sqrt(1/i * (a0 + a1 + a2 + …… a_{i-1}))


做Feature Scaling

- 对于每一个特征算一个均值m出来，然后除掉方差
- x = （x - m） / \sigma

## GD Theory
Taylor Series 泰勒序列

只有lr无穷小的时候才能够保证是向最小的方向进行收敛。

## 学习误差为什么是偏差和方差而产生的，并且推导数学公式

## 过拟合，欠拟合，分别对应bias和variance什么情况

- variance比较大是overfitting，
- bias比较大是underfitting

## 学习鞍点，复习上次任务学习的全局最优和局部最优
解决办法有哪些
梯度下降
学习Mini-Batch与SGD
学习Batch与Mini-Batch，SGD梯度下降的区别
如何根据样本大小选择哪个梯度下降(批量梯度下降，Mini-Batch）
写出SGD和Mini-Batch的代码
学习交叉验证
学习归一化 
学习回归模型评价指标