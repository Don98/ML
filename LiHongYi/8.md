# 李宏毅第八次打卡

# 决策树

## 《李航统计学习》

首先是决策树的模型结构：将数据的属性值作为一个个的结点，根据输入数据的结点的不同的值进入不同的子树，然后到最后判断出结构来。

决策树是通过递归产生结点的：

输入：训练集 $D = ${$(x_1,y_1),(x_2,y_2),\dots,(x_m,y_m)$}

&emsp;&emsp;&emsp;属性集：$A = ${$a_1,a_2,\dot , a_d$};

过程：函数TreeGenerate(D,A)

1. 生成结点node；
2. **if** D 中样本权属于同一个类别C **then**
3. &emsp;&emsp;将node标记为C类叶结点；**return**
4. **end if**
5. **if** $A = \emptyset$ **OR** D中样本在A上的取值相同 **then**
6. &emsp;&emsp;将node标记为叶结点，其类别标记为D中样本数组多的类；**return**
7. **end if**
8. 从A中选择最优化分属性$a_*$;
9. **for** $ a_* $ 
10. &emsp;&emsp;的每一个值 $ a_{*}^v$ **do**
11. &emsp;&emsp;为node生成一个分支；令$D_v$表示D中在$a_*$上取值为
12. &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$a_*^v$的样本子集
13. &emsp;&emsp;**if** $D_v$为空，**then**
14. &emsp;&emsp;&emsp;&emsp;将分支结点标记为叶结点，其类别标记为D中样本最多的类；**return**
15. &emsp;&emsp;**else**
16. &emsp;&emsp;&emsp;&emsp;以$TreeGenerate(D_v,a $ \ {$a_*$}$ )$为分支结点
17. &emsp;&emsp;**end if**
18. **end for**

输出： 以node为根的一颗决策树

三种情形导致递归返回

- 1.当前结点包含的样本全属于同一个类别，无需划分
- 2.当前属性集为空，或是所有样本在所有属性上取值相同，无法划分
- 3.当前结点包含的样本集合为空，不能划分。


## 信息增益

信息熵(information entropy)，度量纯度的最常用的一种指标。

- D：样本集合
- $p_k$第k类样本所占的比例（k = 1，2，……，|y|）

$$Ent(D) = -\sum_{k=1}^{|y|}pklog_2p_k$$

Ent(D)的值越小纯度越高

假设离散属性a有V个可能的取值{$a^1,a^2,\dots , a^V$},如果使用a来对样本集进行划分，那么将会产生V个分支结点，第v个分支结点包含了D中所有在属性a上取值为$a^v$的样本，记为$D^v$,考虑到不同的分支结点锁包含的样本数不同，给分支结点赋予权重$\frac{|D^v|}{|D|}$,即样本数越多的分支结点的影响越大，此时可计算出用属性a对样本集D进行划分所获得的“信息增益”（information gain）

$$Gain(D,a) = Ent(D) - \sum_{v=1}^V \frac{|D^v|}{D} * Ent(D^v)$$

一般而言信息增益越大，意味着使用属性a进行划分的纯度越高，所以可以使用信息增益来进行决策树的划分属性选择，即选择

$a_{*} = $ 

$\mathop{\arg\min}_{a \in A} \ \ Gain(D,a)$

## 信息增益率

信息增益对于取值数目比较多的属性有多偏好，为了减少这种偏好带来的不利影响，C4.5决策算法不直接使用信息增益而是直接使用“增益率”（gain ratio）来选择最优的划分。

$$Gain_ratio(D,a) = \frac{Gain(D,a)}{IV(a)}$$

其中

$$IV(a) = -\sum_{v=1}^V \frac{|D^v|}{D}log_2\frac{D^v}{D}$$

被称为a的“固有值”(intrinsic value)，属性a的可能取值数目越多（也就是V越大），则IV(a)的值通常越大。

增益率可能对数目比较少的有所偏好。所以C4.5不是直接选择增益率最大的候选划分属性，而是使用了一个启发式：先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的。

## ID3算法的优缺点

我们都知道ID3算法是根据某个属性的纯度来进行选择的，但是信息增益对于取值数目比较多的属性有多偏好

## C4.5算法优缺点

这个算法是根据增益率来进行划分，而增益率可能对数目比较少的有所偏好。所以C4.5不是直接选择增益率最大的候选划分属性，而是使用了一个启发式：先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的。这也弥补了对数目比较少的偏好的缺点。


## C4.5算法对于ID3算法的提升。

首先是C4.5采用了增益率而不是ID3的单纯的信息增益，这个可以避免信息增益对于取值数目比较多的属性的偏好的问题，而同时在使用信息增益率的时候也不是直接选择最高的而是先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的。


## C4.5对于连续值上的处理

在这里其实是对连续的数值进行分段，然后再进行处理。


[一份样例的代码](https://github.com/Don98/ML/blob/master/LiHongYi/decision_tree.py)